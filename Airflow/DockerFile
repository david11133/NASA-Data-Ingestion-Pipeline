# Dockerfile for Apache Airflow with custom dependencies for NASA NEO data fetching project.

# Use the official Apache Airflow image
FROM apache/airflow:2.5.0

# Set the Airflow home directory
ENV AIRFLOW_HOME=/opt/airflow

# Switch to the root user for installation of system dependencies
USER root

# Update package lists, upgrade packages, and install necessary tools
RUN apt-get update && \
    apt-get upgrade -y && \
    apt-get install -y git && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Upgrade pip to the latest version
RUN pip install --upgrade pip

# Copy the requirements file into the image
COPY requirements.txt ./

# Install Python dependencies from requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Set the shell to improve error handling and debugging
SHELL ["/bin/bash", "-o", "pipefail", "-e", "-u", "-x", "-c"]

# Optional: If you need to install additional SDKs or tools, you can do it here.
# For example, if using Google Cloud SDK:
# ARG CLOUD_SDK_VERSION=322.0.0
# ENV GCLOUD_HOME=/home/google-cloud-sdk
# ENV PATH="${GCLOUD_HOME}/bin/:${PATH}"
# RUN DOWNLOAD_URL="https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-${CLOUD_SDK_VERSION}-linux-x86_64.tar.gz" \
#     && TMP_DIR="$(mktemp -d)" \
#     && curl -fL "${DOWNLOAD_URL}" --output "${TMP_DIR}/google-cloud-sdk.tar.gz" \
#     && mkdir -p "${GCLOUD_HOME}" \
#     && tar xzf "${TMP_DIR}/google-cloud-sdk.tar.gz" -C "${GCLOUD_HOME}" --strip-components=1 \
#     && "${GCLOUD_HOME}/install.sh" --quiet --bash-completion=false --path-update=false --usage-reporting=false \
#     && rm -rf "${TMP_DIR}" \
#     && gcloud --version

# Set the working directory to AIRFLOW_HOME
WORKDIR $AIRFLOW_HOME

# Switch back to the airflow user for running Airflow tasks
USER $AIRFLOW_UID

# Optionally expose ports or define other configurations as needed
# EXPOSE 8080
