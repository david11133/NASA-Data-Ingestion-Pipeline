<p align="center">
<img height="150" width="150" src="https://cdn.icon-icons.com/icons2/2699/PNG/512/nasa_logo_icon_170926.png"/>
</p>

<h1 align="center">NASA NEO Data Engineering Project</h1>

> [!NOTE]
> This project is currently under development. Features and functionalities are still being added, and the structure may change.

## ğŸ’¡ Overview
This project demonstrates a real-time data streaming and processing pipeline using NASA's NEOWS (Near Earth Object Web Service) APIs to generate dynamic data on near-Earth objects. A Python script fetches data from the NEOWS APIs, publishing it to a Kafka topic for efficient management. We orchestrate this process with Apache Airflow, scheduling the data generation script to run regularly. Spark Structured Streaming is then utilized to consume and modify the data from Kafka, which is ultimately stored in a Cassandra database. All components run within Docker containers, ensuring a consistent and scalable development environment.

## ğŸ“œ Scenario
This project utilizes a database of near-Earth objects (NEOs) sourced from NASA's NEOWS APIs. The database is regularly updated with real-time data on NEOs, including their trajectories and potential hazards. To ensure the data is actionable, it requires automated cleaning and transformation processes to convert it into usable formats with minimal human intervention. The goal is to facilitate continuous monitoring and analysis, providing valuable insights into the risks posed by these celestial objects.

## ğŸ“Š Data
The data for this project is sourced from NASA's NEOWS APIs, which provide extensive information on near-Earth objects.

```json
{
    "Date": "2024-09-27",
    "ID": "54483136",
    "Name": "(2024 SY2)",
    "Estimated Diameter (km)": {
        "min": 0.0173362426,
        "max": 0.038765017
    },
    "Is Potentially Hazardous": false,
    "Close Approach Date": "2024-09-27",
    "Relative Velocity (km/s)": "8.7240830597",
    "Miss Distance (km)": "7580603.623832072"
}
```
  

## ğŸ—ï¸ Data Architecture
![System Architecture](https://github.com/david11133/NASA-Data-Ingestion-Pipeline/raw/main/docs/data%20architecture.drawio.svg)

## ğŸ›  Technologies Used
- Cloud - [**AWS**](https://aws.amazon.com/)
- Containerization - [**Docker**](https://www.docker.com), [**Docker Compose**](https://docs.docker.com/compose/)
- Stream Processing - [**Kafka**](https://kafka.apache.org)
- Orchestration - [**Airflow**](https://airflow.apache.org)
- Transformation - [**Spark**](https://spark.apache.org)
- Data Lake - [**Cassandra**](https://cassandra.apache.org/_/index.html)
- Language - [**Python**](https://www.python.org)

## ğŸ”§ Project Structure

```graphql
NASA-Data-Ingestion-Pipeline/

â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚ Â  â”‚ Â  â”œâ”€â”€ nasa_kafka_stream.py Â  Â  Â  Â  # Airflow DAG for Kafka streaming
â”‚ Â  â”œâ”€â”€ scripts/
â”‚ Â  â”‚ Â  â””â”€â”€ entrypoint.sh Â  Â  Â  Â  Â  Â  Â  Â # Entrypoint for Airflow Docker container
â”‚ Â  â””â”€â”€ .env
â”‚
â”œâ”€â”€ spark/
â”‚ Â  â”œâ”€â”€ scripts/
â”‚ Â  â”‚ Â  â””â”€â”€ spark_stream.py Â  Â  Â  Â  Â  Â  Â # Spark streaming job
â”‚ Â  â”œâ”€â”€ config/
â”‚ Â  â”‚ Â  â””â”€â”€ spark-defaults.conf Â  Â  Â  Â  Â # Spark defaults
â”‚ Â  â””â”€â”€ examples/
â”‚ Â  Â  Â  â””â”€â”€ example_spark_job.py Â  Â  Â  Â  # Example Spark job
â”‚
â”œâ”€â”€ requirements.txt Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Python dependencies for Airflow
â”‚
â”œâ”€â”€ docker-compose.yml Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Docker Compose file for services
â”‚
â”œâ”€â”€ docs/
â”‚ Â  â”œâ”€â”€ architecture.md Â  Â  Â  Â  Â  Â  Â  Â  Â # Overview of system architecture
â”‚ Â  â”œâ”€â”€ setup_guide.md Â  Â  Â  Â  Â  Â  Â  Â  Â  # Instructions for setting up the project
â”‚ Â  â”œâ”€â”€ api_documentation.md Â  Â  Â  Â  Â  Â  # API details for the user data endpoint
â”‚ Â  â””â”€â”€ technology_overview.md Â  Â  Â  Â  Â  # Descriptions of all technologies used
â”‚
â””â”€â”€ README.md Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Main project overview and instructions
```

##  Getting Started
1. Clone the repository:
    ```bash
    git clone https://github.com/david11133/NASA-Data-Ingestion-Pipeline
    ```

2. Navigate to the project directory:
    ```bash
    cd NASA-Data-Ingestion-Pipeline
    ```

3. Run Docker Compose to spin up the services:
    ```bash
    docker-compose up
    ```

## ğŸŒ Contact
For any questions or contributions, please reach out to [davidnady4yad@gmail.com].